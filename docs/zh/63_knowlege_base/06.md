# 片段向量

[[toc]]

## 项目目标

本项目旨在构建一个功能完备的 RAG（Retrieval-Augmented Generation）系统，主要目标包括：

- **知识库管理**：支持创建、更新和删除知识库，便于用户高效维护内容。
- **文档处理**：包括文档的拆分、片段的向量化处理，以提升检索效率和准确性。
- **问答系统**：提供高效的向量检索和实时生成回答的能力，支持复杂汇总类问题的处理。
- **系统优化**：通过统计分析和推理问答调试，不断优化系统性能和用户体验。

## 系统核心概念

在 RAG 系统中，以下是几个核心概念：

- **应用**：知识库的集合。每个应用可以自定义提示词，以满足不同的个性化需求。
- **知识库**：由多个文档组成，便于用户对内容进行分类和管理。
- **文档**：系统中对应的真实文档内容。
- **片段**：文档经过拆分后的最小内容单元，用于更高效的处理和检索。

## 功能实现步骤

1. **数据库设计** [查看 01.md](./01.md)  
   设计并实现项目所需的数据表结构与数据库方案，为后续的数据操作打下坚实基础。

2. **用户登录** [查看 02.md](./02.md)  
   实现了安全可靠的用户认证系统，保护用户数据并限制未经授权的访问。

3. **模型管理** [查看 03.md](./03.md)  
   支持针对不同平台的模型（如 OpenAI、Google Gemini、Claude）进行管理与配置。

4. **知识库管理** [查看 04.md](./04.md)  
   提供创建、更新及删除知识库的功能，方便用户维护与管理文档内容。

5. **文档拆分** [查看 05.md](./05.md)  
   可将文档拆分为多个片段，便于后续向量化和检索操作。

6. **片段向量** [查看 06.md](./06.md)  
   将文本片段进行向量化处理，以便进行语义相似度计算及高效检索。

7. **命中率测试** [查看 07.md](./07.md)  
   通过语义相似度和 Top-N 算法，检索并返回与用户问题最相关的文档片段，用于评估检索的准确性。

8. **文档管理** [查看 08.md](./08.md)  
   提供上传和管理文档的功能，上传后可自动拆分为片段便于进一步处理。

9. **片段管理** [查看 09.md](./09.md)  
   允许对已拆分的片段进行增、删、改、查等操作，确保内容更新灵活可控。

10. **问题管理** [查看 10.md](./10.md)  
    为片段指定相关问题，以提升检索时的准确性与关联度。

11. **应用管理** [查看 11.md](./11.md)  
    提供创建和配置应用（智能体）的功能，并可关联指定模型和知识库。

12. **向量检索** [查看 12.md](./12.md)  
    基于语义相似度，在知识库中高效检索与用户问题最匹配的片段。

13. **推理问答调试** [查看 13.md](./13.md)  
    提供检索与问答性能的评估工具，帮助开发者进行系统优化与调试。

14. **对话问答** [查看 14.md](./14.md)  
    为用户提供友好的人机交互界面，结合检索到的片段与用户问题实时生成回答。

15. **统计分析** [查看 15.md](./15.md)  
    对用户的提问与系统回答进行数据化分析，并以可视化图表的形式呈现系统使用情况。

16. **用户管理** [查看 16.md](./16.md)  
    提供多用户管理功能，包括用户的增删改查及权限控制。

17. **API 管理** [查看 17.md](./17.md)  
    对外提供标准化 API，便于外部系统集成和调用本系统的功能。

18. **存储文件到 S3** [查看 18.md](./18.md)  
    将用户上传的文件存储至 S3 等对象存储平台，提升文件管理的灵活性与可扩展性。

19. **文档解析优化** [查看 19.md](./19.md)  
    介绍与对比常见的文档解析方案，并提供提升文档解析速度和准确性的优化建议。

20. **片段汇总** [查看 20.md](./20.md)  
    对片段内容进行汇总，以提升总结类问题的查询与回答效率。

21. **文档多分块与检索** [查看 21.md](./21.md)  
    将片段进一步拆分为句子并进行向量检索，提升检索的准确度与灵活度。

22. **多文档支持** [查看 22.md](./22.md)  
    兼容多种文档格式，包括 `.doc`, `.docx`, `.xls`, `.xlsx`, `.ppt`, `.pptx` 等。

23. **对话日志** [查看 23.md](./23.md)  
    记录并展示对话日志，用于后续分析和问题回溯。

24. **检索性能优化** [查看 24.md](./24.md)  
    提供整库扫描和分区检索等多种方式，进一步提高检索速度和效率。

25. **Milvus** [查看 25.md](./25.md)  
    将向量数据库切换至 Milvus，以在大规模向量检索场景中获得更佳的性能与可扩展性。

26. **文档解析方案和费用对比** [查看 26.md](./26.md)  
    对比不同文档解析方案在成本、速度、稳定性等方面的差异，为用户提供更加经济高效的选择。

27. **爬取网页数据** [查看 27.md](./27.md)  
     支持从网页中抓取所需内容，后续处理流程与本地文档一致：分段、向量化、存储与检索。  
    我们已经完成了数据库设计、用户登录、知识库管理以及文件拆分功能。接下来，我们将实现片段向量化功能。

## 概述

片段向量化需要调用大型语言模型。为了减少向量模型的调用次数，我们将向量模型的返回结果缓存到 `max_kb_embedding_cache` 表中。以下是相关接口及代码的实现细节。

## 接口文档

### 批量处理文档向量

**请求方法**: `POST`

**请求路径**: `/api/dataset/{id}/document/_bach`

**请求示例**:

```json
[
  {
    "name": "ICS111_31391_Miller_Syllabus_F24.pdf",
    "id": "file_id",
    "paragraphs": [
      {
        "title": "",
        "content": "ICS 111- Introduction to Computer Science I, 31391\nFall"
      }
    ]
  }
]
```

**响应示例**:

```json
{
  "code": 200,
  "message": "success",
  "data": [
    {
      "create_time": "2024-11-05T14:24:12.315",
      "update_time": "2024-11-05T14:24:12.315",
      "id": "xxx",
      "name": "ICS111_31391_Miller_Syllabus_F24.pdf",
      "char_length": 20755,
      "status": "3",
      "is_active": true,
      "type": "0",
      "meta": {},
      "dataset_id": "xxx",
      "hit_handling_method": "optimization",
      "directly_return_similarity": 0.9,
      "paragraph_count": 6
    }
  ]
}
```

## 代码实现

### 控制器层

```java
package com.litongjava.maxkb.controller;

import com.litongjava.annotation.Delete;
import com.litongjava.annotation.Get;
import com.litongjava.annotation.Post;
import com.litongjava.annotation.Put;
import com.litongjava.annotation.RequestPath;
import com.litongjava.jfinal.aop.Aop;
import com.litongjava.maxkb.service.kb.MaxKbDatasetHitTestService;
import com.litongjava.maxkb.service.kb.MaxKbDatasetService;
import com.litongjava.maxkb.service.kb.MaxKbDocumentService;
import com.litongjava.maxkb.service.kb.MaxKbParagraphServcie;
import com.litongjava.maxkb.service.kb.MaxKbParagraphSplitService;
import com.litongjava.maxkb.service.kb.MaxKbProblemService;
import com.litongjava.maxkb.vo.KbDatasetModel;
import com.litongjava.maxkb.vo.Paragraph;
import com.litongjava.maxkb.vo.ParagraphBatchVo;
import com.litongjava.maxkb.vo.ProbrolemCreateBatch;
import com.litongjava.model.result.ResultVo;
import com.litongjava.tio.boot.http.TioRequestContext;
import com.litongjava.tio.http.common.HttpRequest;
import com.litongjava.tio.utils.json.JsonUtils;

@RequestPath("/api/dataset")
public class AapiDatasetController {

  @Post("/{id}/document/_bach")
  public ResultVo batch(Long id, HttpRequest request) {
    String bodyString = request.getBodyString();
    Long userId = TioRequestContext.getUserIdLong();
    List<ParagraphBatchVo> list = JsonUtils.parseArray(bodyString, ParagraphBatchVo.class);
    return Aop.get(MaxKbParagraphSplitService.class).batch(userId, id, list);
  }
}
```

### 服务层

#### DatasetDocumentVectorService

```java
package com.litongjava.maxkb.service;

import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.CompletionService;
import java.util.concurrent.ExecutorCompletionService;
import java.util.concurrent.Future;

import org.postgresql.util.PGobject;

import com.jfinal.kit.Kv;
import com.litongjava.db.TableInput;
import com.litongjava.db.TableResult;
import com.litongjava.db.activerecord.Db;
import com.litongjava.db.activerecord.Row;
import com.litongjava.jfinal.aop.Aop;
import com.litongjava.maxkb.constant.TableNames;
import com.litongjava.maxkb.utils.ExecutorServiceUtils;
import com.litongjava.maxkb.vo.DocumentBatchVo;
import com.litongjava.maxkb.vo.Paragraph;
import com.litongjava.model.result.ResultVo;
import com.litongjava.table.services.ApiTable;
import com.litongjava.tio.utils.crypto.Md5Utils;
import com.litongjava.tio.utils.hutool.FilenameUtils;
import com.litongjava.tio.utils.snowflake.SnowflakeIdUtils;

public class DatasetDocumentVectorService {

  public ResultVo batch(Long userId, Long dataset_id, List<DocumentBatchVo> list) {

    TableInput tableInput = new TableInput();
    tableInput.set("id", dataset_id);
    if (!userId.equals(1L)) {
      tableInput.set("user_id", userId);
    }

    TableResult<Row> result = ApiTable.get(TableNames.max_kb_dataset, tableInput);

    Row dataset = result.getData();
    if (dataset == null) {
      return ResultVo.fail("Dataset not found.");
    }

    Long embedding_mode_id = dataset.getLong("embedding_mode_id");
    String sqlModelName = String.format("SELECT model_name FROM %s WHERE id = ?", TableNames.max_kb_model);
    String modelName = Db.queryStr(sqlModelName, embedding_mode_id);

    String sqlDocumentId = String.format("SELECT id FROM %s WHERE user_id = ? AND file_id = ?", TableNames.max_kb_document);

    List<Kv> kvs = new ArrayList<>();
    CompletionService<Row> completionService = new ExecutorCompletionService<>(ExecutorServiceUtils.getExecutorService());

    for (DocumentBatchVo documentBatchVo : list) {
      Long fileId = documentBatchVo.getId();
      String filename = documentBatchVo.getName();
      Long documentId = Db.queryLong(sqlDocumentId, userId, fileId);

      List<Paragraph> paragraphs = documentBatchVo.getParagraphs();
      int char_length = 0;
      for (Paragraph p : paragraphs) {
        if (p.getContent() != null) {
          char_length += p.getContent().length();
        }
      }
      String type = FilenameUtils.getSuffix(filename);

      if (documentId == null) {
        documentId = SnowflakeIdUtils.id();
        Row row = Row.by("id", documentId).set("file_id", fileId).set("user_id", userId).set("name", filename).set("char_length", char_length).set("status", "1").set("is_active", true)
            .set("type", type).set("dataset_id", dataset_id).set("paragraph_count", paragraphs.size()).set("hit_handling_method", "optimization").set("directly_return_similarity", 0.9);
        Db.save(TableNames.max_kb_document, row);
        Kv kv = row.toKv();
        kvs.add(kv);
      } else {
        Row existingRecord = Db.findById(TableNames.max_kb_document, documentId);
        if (existingRecord != null) {
          Kv kv = existingRecord.toKv();
          kvs.add(kv);
        } else {
          // Handle the case where documentId is provided but the row does not exist
          return ResultVo.fail("Document not found for ID: " + documentId);
        }
      }

      MaxKbEmbeddingService maxKbEmbeddingService = Aop.get(MaxKbEmbeddingService.class);

      List<Future<Row>> futures = new ArrayList<>();
      final long documentIdFinal = documentId;
      for (Paragraph p : paragraphs) {
        futures.add(completionService.submit(() -> {
          String title = p.getTitle();
          String content = p.getContent();
          PGobject vector = maxKbEmbeddingService.getVector(content, modelName);
          return Row.by("id", SnowflakeIdUtils.id())
              //
              .set("source_id", fileId)
              //
              .set("source_type", type)
              //
              .set("title", title)
              //
              .set("content", content)
              //
              .set("md5", Md5Utils.getMD5(content))
              //
              .set("status", "1")
              //
              .set("hit_num", 0)
              //
              .set("is_active", true).set("dataset_id", dataset_id).set("document_id", documentIdFinal)
              //
              .set("embedding", vector);
        }));
      }

      List<Row> batchRecord = new ArrayList<>(paragraphs.size());
      for (int i = 0; i < paragraphs.size(); i++) {
        try {
          Future<Row> future = completionService.take();
          Row row = future.get();
          if (row != null) {
            batchRecord.add(row);
          }
        } catch (Exception e) {
          e.printStackTrace();
        }
      }

      boolean transactionSuccess = Db.tx(() -> {
        Db.delete(TableNames.max_kb_paragraph, Row.by("document_id", documentIdFinal));
        Db.batchSave(TableNames.max_kb_paragraph, batchRecord, 2000);
        return true;
      });

      if (!transactionSuccess) {
        return ResultVo.fail("Transaction failed while saving paragraphs for document ID: " + documentIdFinal);
      }
    }

    return ResultVo.ok(kvs);
  }
}
```

#### MaxKbEmbeddingService

```java
package com.litongjava.maxkb.service;

import java.util.Arrays;

import org.postgresql.util.PGobject;

import com.litongjava.db.activerecord.Db;
import com.litongjava.db.activerecord.Row;
import com.litongjava.db.utils.PgVectorUtils;
import com.litongjava.maxkb.constant.TableNames;
import com.litongjava.openai.client.OpenAiClient;
import com.litongjava.tio.utils.crypto.Md5Utils;
import com.litongjava.tio.utils.snowflake.SnowflakeIdUtils;

public class MaxKbEmbeddingService {
  private final Object vectorLock = new Object();
  private final Object writeLock = new Object();

  public PGobject getVector(String text, String model) {
    String v = null;
    String md5 = Md5Utils.getMD5(text);
    String sql = String.format("select v from %s where md5=? and m=?", TableNames.max_kb_embedding_cache);
    PGobject pGobject = Db.queryFirst(sql, md5, model);

    if (pGobject == null) {
      float[] embeddingArray = null;
      try {
        embeddingArray = OpenAiClient.embeddingArray(text, model);
      } catch (Exception e) {
        try {
          embeddingArray = OpenAiClient.embeddingArray(text, model);
        } catch (Exception e1) {
          embeddingArray = OpenAiClient.embeddingArray(text, model);
        }
      }

      String string = Arrays.toString(embeddingArray);
      long id = SnowflakeIdUtils.id();
      v = (String) string;
      pGobject = PgVectorUtils.getPgVector(v);
      Row saveRecord = new Row().set("t", text).set("v", pGobject).set("id", id).set("md5", md5)
          //
          .set("m", model);
      synchronized (writeLock) {
        Db.save(TableNames.max_kb_embedding_cache, saveRecord);
      }
    }
    return pGobject;
  }

  public Long getVectorId(String text, String model) {
    String md5 = Md5Utils.getMD5(text);
    String sql = String.format("select id from %s where md5=? and m=?", TableNames.max_kb_embedding_cache);
    Long id = Db.queryLong(sql, md5, model);

    if (id == null) {
      float[] embeddingArray = null;
      synchronized (vectorLock) {
        embeddingArray = OpenAiClient.embeddingArray(text, model);
      }

      String vString = Arrays.toString(embeddingArray);
      id = SnowflakeIdUtils.id();
      PGobject pGobject = PgVectorUtils.getPgVector(vString);
      Row saveRecord = new Row().set("t", text).set("v", pGobject).set("id", id).set("md5", md5)
          //
          .set("m", model);
      synchronized (writeLock) {
        Db.save(TableNames.max_kb_embedding_cache, saveRecord);
      }
    }
    return id;
  }
}
```

## 说明

1. **缓存机制**: 为了优化性能，`MaxKbEmbeddingService` 在生成向量时会先查询 `max_kb_embedding_cache` 表。如果缓存中存在对应的向量，则直接使用缓存；否则，调用 OpenAI 接口生成向量，并将结果缓存到数据库中。

2. **线程安全**: 在 `MaxKbEmbeddingService` 中，`vectorLock` 和 `writeLock` 用于确保向量生成和缓存写入过程的线程安全，避免并发冲突。

3. **事务管理**: 在 `DatasetDocumentVectorService` 中，批量保存段落记录时使用事务管理，确保数据的一致性。

4. **唯一标识**: 使用 `SnowflakeIdUtils` 生成唯一的 ID，确保记录的唯一性。

5. **MD5 校验**: 使用 MD5 对段落内容进行校验，以便快速查询和缓存。

## 总结

通过以上实现，我们完成了片段向量化的功能，利用缓存机制有效减少了对向量模型的调用次数，提升了系统的整体性能和响应速度。后续可以根据需求进一步优化缓存策略和并发处理机制。
