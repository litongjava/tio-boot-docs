# ai-search 搜索、重排与读取内容

[[toc]]
[[toc]]
本文档详细介绍了一个基于 Java 的搜索与网页爬取系统的实现过程。该系统由四个主要模块组成：

1. **搜索请求处理器**
2. **搜索服务**
3. **AI 重排服务**
4. **Playwright 页面爬取服务 获取 Jina Reader API 内容读取**

系统整体流程为：

- 用户发送搜索请求。
- 系统调用搜索引擎 API 获取初步搜索结果。
- 利用大模型进行重排和过滤，选出最有可能包含答案的结果。
- 根据需要，使用 Playwright 或 Jina Reader API 读取网页详细内容，并将最终结果返回给用户。

本文档不仅展示了完整代码，还提供了每个模块的功能解释和实现细节说明。

---

## 1. 搜索请求处理器

`SearxngSearchHandler` 类负责接收 HTTP 请求，并将请求参数传递给搜索服务进行处理。代码如下：

```java
package com.litongjava.perplexica.handler;

import java.util.List;

import com.litongjava.jfinal.aop.Aop;
import com.litongjava.model.web.WebPageContent;
import com.litongjava.perplexica.services.SearxngSearchService;
import com.litongjava.searxng.SearxngSearchParam;
import com.litongjava.tio.boot.http.TioRequestContext;
import com.litongjava.tio.http.common.HttpRequest;
import com.litongjava.tio.http.common.HttpResponse;
import com.litongjava.tio.utils.environment.EnvUtils;

import lombok.extern.slf4j.Slf4j;

@Slf4j
public class SearxngSearchHandler {

  public HttpResponse search(HttpRequest request) {
    log.info("request line:{}", request.requestLine.toString());
    // 从请求中获取各参数
    String format = "json";
    String q = request.getString("q");
    String language = request.getString("language");
    String categories = request.getString("categories");
    String engines = request.getString("engines");
    Integer pageno = request.getInt("pageno");
    String time_range = request.getString("time_range");
    Integer safesearch = request.getInt("safesearch");
    String autocomplete = request.getString("autocomplete");
    String locale = request.getString("locale");
    Boolean no_cache = request.getBoolean("no_cache");
    String theme = request.getString("theme");
    //自有参数
    Integer limit = request.getInt("limit");
    Boolean fetch = request.getBoolean("fetch");

    // 创建并设置 SearxngSearchParam 对象的属性
    SearxngSearchParam param = new SearxngSearchParam();
    param.setFormat(format);
    param.setQ(q);
    param.setLanguage(language);
    param.setCategories(categories);
    param.setEngines(engines);
    param.setPageno(pageno);
    param.setTime_range(time_range);
    param.setSafesearch(safesearch);
    param.setAutocomplete(autocomplete);
    param.setLocale(locale);
    param.setNo_cache(no_cache);
    param.setTheme(theme);

    String baseUrl = EnvUtils.getStr("SEARXNG_API_BASE");
    String endpoint = baseUrl + "/search";

    // 使用封装后的参数调用服务
    List<WebPageContent> pages = Aop.get(SearxngSearchService.class).search(endpoint, param, fetch, limit);
    return TioRequestContext.getResponse().setJson(pages);
  }
}
```

### 说明

- **参数解析**：从 HTTP 请求中提取查询参数，如搜索关键词 (`q`)、语言、类别、引擎、页码等。
- **参数封装**：将提取的参数封装到 `SearxngSearchParam` 对象中，便于后续调用搜索服务。
- **搜索服务调用**：通过 AOP 获取 `SearxngSearchService` 实例，调用其 `search` 方法，并传入是否抓取 (`fetch`) 与限制数量 (`limit`) 参数。
- **响应处理**：将搜索服务返回的结果封装成 JSON 并返回给客户端。

---

## 2. 搜索服务

`SearxngSearchService` 类负责调用外部搜索 API，处理搜索结果，并根据需求进一步调用 AI 过滤和页面爬取服务。

```java
package com.litongjava.perplexica.services;

import java.util.ArrayList;
import java.util.List;

import com.litongjava.jfinal.aop.Aop;
import com.litongjava.model.web.WebPageContent;
import com.litongjava.searxng.SearxngResult;
import com.litongjava.searxng.SearxngSearchClient;
import com.litongjava.searxng.SearxngSearchParam;
import com.litongjava.searxng.SearxngSearchResponse;

public class SearxngSearchService {

  public List<WebPageContent> search(String endpoint, SearxngSearchParam param, Boolean fetch, Integer limit) {
    SearxngSearchResponse searchResponse = SearxngSearchClient.search(endpoint, param);
    List<SearxngResult> results = searchResponse.getResults();
    List<WebPageContent> pages = new ArrayList<>();
    for (SearxngResult searxngResult : results) {
      String title = searxngResult.getTitle();
      String url = searxngResult.getUrl();
      String content = searxngResult.getContent();
      pages.add(new WebPageContent(title, url, content));
    }
    if (fetch != null && fetch) {
      if (limit == null) {
        pages = Aop.get(PlaywrightService.class).spiderAsync(pages);
      } else {
        pages = Aop.get(AiRankerService.class).filter(pages, param.getQ(), limit);
      }

      //pages = Aop.get(PlaywrightService.class).spiderAsync(pages);
      //或者替换为使用 Jina Reader API 读取页面内容
      pages = Aop.get(JinaReaderService.class).spiderAsync(pages);
    }
    return pages;
  }
}
```

### 说明

- **搜索调用**：调用 `SearxngSearchClient.search` 方法，根据用户传入参数调用搜索 API，获取搜索结果。
- **结果转换**：将返回的 `SearxngResult` 对象转换为 `WebPageConteont` 对象列表，便于后续统一处理。
- **条件处理**：
  - 若 `fetch` 参数为 `true` 且未指定 `limit`，直接对所有页面进行 Playwright 抓取；
  - 若指定了 `limit`，则先调用 AI 过滤服务过滤出最相关的页面，再使用 Playwright 进行页面内容抓取。
  - 注：可根据实际需求选择使用 Playwright 或 Jina Reader API 获取页面内容。
- **响应构造**：将最终处理后的页面列表封装在 `RespBodyVo` 对象中返回给调用方。

---

## 3. AI 排序服务

### WebSearchSelectPrompt.txt

WebSearchSelectPrompt.txt 重拍提示词

```txt
You are a college student assistant.
I will provide you with the question asked by the user along with a list of search results returned by the search engine.
You need to output the #(limit) results that is most likely to contain the answer.
If you dont' konw, you need to return `not_found` as the response.

Please only output the title and link and  provide the they between the XML tags <output> and </output>. For example:
<output>
Academic-Calendar-2024-25.pdf~~https://www.sjsu.edu/provost/docs/Academic-Calendar-2024-25.pdf
2024-2025 | Class Schedules~~https://www.<b>sjsu</b>.edu/classes/calendar/2024-2025.php
First Day® Solutions | Bursar's Office~~https://www.<b>sjsu</b>.edu/bursar/our-services/first-<b>day</b>-solutions.php
</output>

question: #(quesiton)
search_result:#(search_result)
```

### AiRankerService

`AiRankerService` 类利用大模型对搜索结果进行过滤，选择最相关的结果。主要步骤如下：生成提示词、调用 Gemini AI 服务、解析返回结果。

```java
package com.litongjava.perplexica.services;

import java.util.ArrayList;
import java.util.List;

import com.jfinal.kit.Kv;
import com.litongjava.jfinal.aop.Aop;
import com.litongjava.model.web.WebPageContent;
import com.litongjava.template.PromptEngine;
import com.litongjava.tio.utils.json.JsonUtils;
import com.litongjava.tio.utils.tag.TagUtils;

// @Slf4j
public class AiRankerService {
  public List<WebPageContent> filter(List<WebPageContent> pages, String question, Integer limit) {
    Kv kv = Kv.by("limit", limit).set("quesiton", question).set("search_result", JsonUtils.toJson(pages));
    String fileName = "WebSearchSelectPrompt.txt";
    String prompt = PromptEngine.renderToString(fileName, kv);
    //log.info("WebSearchSelectPrompt:{}", prompt);

    String selectResultContent = Aop.get(GeminiService.class).generate(prompt);
    List<String> outputs = TagUtils.extractOutput(selectResultContent);
    String titleAndLinks = outputs.get(0);
    if ("not_found".equals(titleAndLinks)) {
      return null;
    }

    //4.send to client
    String[] split = titleAndLinks.split("\n");
    List<WebPageContent> citationList = new ArrayList<>();
    for (int i = 0; i < split.length; i++) {
      String[] split2 = split[i].split("~~");
      citationList.add(new WebPageContent(split2[0], split2[1]));
    }
    return citationList;
  }
}
```

### GeminiService 实现

```java
package com.litongjava.maxkb.service.api;

import com.litongjava.gemini.GeminiClient;
import com.litongjava.gemini.GoogleGeminiModels;
import com.litongjava.openai.chat.OpenAiChatRequestVo;
import com.litongjava.openai.client.OpenAiClient;
import com.litongjava.openai.consts.OpenAiConstants;
import com.litongjava.tio.utils.environment.EnvUtils;

import lombok.extern.slf4j.Slf4j;
import okhttp3.Call;
import okhttp3.Callback;

@Slf4j
public class GeminiService {
  public String generate(String prompt) {
    String apiKey = EnvUtils.get("GEMINI_API_KEY");
    if (EnvUtils.isDev()) {
      log.info("api key:{}", apiKey);
    }
    return GeminiClient.chatWithModel(apiKey, GoogleGeminiModels.GEMINI_2_0_FLASH_EXP, "user", prompt);
    // 或者使用 OpenAiClient 作为备用实现
    // return OpenAiClient.chatWithModel(OpenAiConstants.GEMINI_OPENAI_API_BASE, apiKey, GoogleGeminiModels.GEMINI_2_0_FLASH_EXP, "user", prompt);
  }

  public Call stream(OpenAiChatRequestVo chatRequestVo, Callback callback) {
    String apiKey = EnvUtils.get("GEMINI_API_KEY");
    Call call = OpenAiClient.chatCompletions(OpenAiConstants.GEMINI_OPENAI_API_BASE, apiKey, chatRequestVo, callback);
    return call;
  }
}
```

### 说明

- **提示词生成**：利用模板 `WebSearchSelectPrompt.txt` 生成提示词，该文件中定义了如何格式化问题与搜索结果，供 AI 模型选择最相关的结果。
- **AI 调用**：通过 `GeminiService.generate` 方法将提示词发送给大模型，获取其返回的选择结果。
- **结果解析**：从返回内容中提取 XML 标签 `<output>` 中的数据，并解析成标题与链接的列表。

---

## 4. 网页内容读取服务

在搜索返回的结果中，通常只包含网页标题、描述与 URL，而不包含网页实际内容。为了让大模型更好地回答用户问题，需要进一步获取网页完整内容。本系统提供两种方案：

### 4.1 Playwright 页面爬取

通过 `PlaywrightService` 模块，可以利用 Playwright 工具对指定 URL 进行异步爬取，从而获取网页详细内容。其调用方式如下：

```java
// 示例：对获取的页面列表进行 Playwright 抓取
pages = Aop.get(PlaywrightService.class).spiderAsync(pages);
```

该服务会针对每个页面，利用 Playwright 模拟浏览器环境加载页面并提取网页内容，适用于需要处理复杂 JavaScript 渲染页面的场景。

### 4.2 Jina Reader API 读取网页内容

Jina Reader API 专为从网页中提取详细内容而设计。它支持通过 HTTP 请求或 Java 客户端方式读取指定网页的完整内容，步骤如下：

#### 请求示例

你可以使用 `curl` 命令直接请求 Jina Reader API 读取页面内容：

```bash
curl https://r.jina.ai/https://www.tio-boot.com/zh/01_tio-boot%20%E7%AE%80%E4%BB%8B/02.html \
  -H "Authorization: Bearer jina_cf99cd628dd34559b21d1f967bfe6cceGqIE6CtNk0JmiZ2sslSu77E3FcYR" \
  -H "X-Respond-With: readerlm-v2"
```

**说明：**

- 请求 URL 由 `https://r.jina.ai/` 与目标网页 URL 拼接而成。
- 请求头 `Authorization` 包含 API Key；
- 请求头 `X-Respond-With` 用于指定返回数据格式或版本（例如 `readerlm-v2`）。

> **延迟说明**
>
> - 使用 `readerlm-v2` 版本时，读取延迟约 54 秒。
> - 不使用该版本时，延迟大约 2.2 秒。

#### 返回数据示例

成功请求后，API 会返回如下格式的数据：

```markdown
Title: Example Domain

URL Source: https://example.com/

Markdown Content:
This domain is for use in illustrative examples in documents. You may use this domain in literature without prior coordination or asking for permission.
[More information...](https://www.iana.org/domains/example)
```

返回结果中包括网页标题、原始 URL 以及页面内容（Markdown 格式）。

#### Java 客户端示例

对于 Java 开发者，可直接调用内置的 `JinaReaderClient` 读取网页内容。示例代码如下：

```java
import org.junit.Test;
import com.litongjava.jian.reader.JinaReaderClient;
import com.litongjava.tio.utils.environment.EnvUtils;

public class JinaReaderClientTest {

  @Test
  public void test() {
    // 加载环境变量配置（例如：JINA_API_KEY）
    EnvUtils.load();
    // 读取指定 URL 的网页内容
    String result = JinaReaderClient.read("https://www.tio-boot.com/zh/01_tio-boot%20%E7%AE%80%E4%BB%8B/02.html");
    // 输出读取结果
    System.out.println(result);
  }
}
```

#### JinaReaderService

在实际业务逻辑中，封装一个 `JinaReaderService`，采用多线程的方式对过滤后的页面列表进行内容读取，例如：

```java
package com.litongjava.perplexica.services;

import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.Future;
import java.util.concurrent.locks.Lock;

import com.google.common.util.concurrent.Striped;
import com.litongjava.db.activerecord.Db;
import com.litongjava.db.activerecord.Row;
import com.litongjava.jian.reader.JinaReaderClient;
import com.litongjava.model.web.WebPageContent;
import com.litongjava.tio.utils.hutool.StrUtil;
import com.litongjava.tio.utils.snowflake.SnowflakeIdUtils;
import com.litongjava.tio.utils.thread.TioThreadUtils;

import lombok.extern.slf4j.Slf4j;

@Slf4j
public class JinaReaderService {
  public static final String cache_table_name = "web_page_cache";
  //使用Guava的Striped锁，设置256个锁段
  private static final Striped<Lock> stripedLocks = Striped.lock(256);

  public List<WebPageContent> spiderAsync(List<WebPageContent> pages) {
    List<Future<String>> futures = new ArrayList<>();

    for (int i = 0; i < pages.size(); i++) {
      String link = pages.get(i).getUrl();

      Future<String> future = TioThreadUtils.submit(() -> {
        return getPageContent(link);
      });
      futures.add(i, future);
    }
    for (int i = 0; i < pages.size(); i++) {
      Future<String> future = futures.get(i);
      try {
        String result = future.get();
        if (StrUtil.isNotBlank(result)) {
          pages.get(i).setContent(result);
        }
      } catch (InterruptedException | ExecutionException e) {
        log.error("Error retrieving task result: {}", e.getMessage(), e);
      }
    }
    return pages;
  }

  private String getPageContent(String link) {
    // 首先检查数据库中是否已存在该页面内容
    if (Db.exists(cache_table_name, "url", link)) {
      // 假设 content 字段存储了页面内容
      return Db.queryStr("SELECT markdown FROM " + cache_table_name + " WHERE url = ?", link);
    }

    // 获取与链接对应的锁并锁定
    Lock lock = stripedLocks.get(link);
    lock.lock();
    try {
      // 再次检查，防止其他线程已生成内容
      if (Db.exists(cache_table_name, "url", link)) {
        return Db.queryStr("SELECT markdown FROM " + cache_table_name + " WHERE url = ?", link);
      }
      // 使用 Jina Reader Client 获取页面内容
      String markdown = JinaReaderClient.read(link);
      // 将获取到的页面内容保存到数据库
      if (markdown != null && !markdown.isEmpty()) {
        // 构造数据库实体或使用直接 SQL 插入
        Row newRow = new Row();
        newRow.set("id", SnowflakeIdUtils.id()).set("url", link)
            //
            .set("markdown", markdown);
        Db.save(cache_table_name, newRow);
      }
      return markdown;
    } finally {
      lock.unlock();
    }
  }
}

```

### 说明

- **Jina Reader API 优点**：提供便捷的方式获取网页完整内容，适用于大模型应用中的问答需求。
- **使用场景**：当需要高效地读取并解析网页文本内容时，可选择 Jina Reader API 替代或补充 Playwright 方案。

---

### 请求示例

```curl
curl --location --request POST 'http://localhost/api/v1/search' \
--header 'User-Agent: Apifox/1.0.0 (https://apifox.com)' \
--header 'Accept: */*' \
--header 'Host: localhost' \
--header 'Connection: keep-alive' \
--header 'Content-Type: application/x-www-form-urlencoded' \
--data-urlencode 'q=Advertising, Area of Specialization in Creative Track, BS (2024-2025) 4 year in sjsu' \
--data-urlencode 'language=auto' \
--data-urlencode 'safesearch=0' \
--data-urlencode 'categories=general' \
--data-urlencode 'fetch=true' \
--data-urlencode 'limit=5'
```

没有命中缓存的延迟是 12s,命中缓存的延迟是 3s

## 5. 总结

本文档介绍了一个基于 Java 的搜索与网页爬取系统的完整实现流程。系统主要包含以下几个步骤：

1. **搜索请求处理**：解析 HTTP 请求参数，将其封装为搜索参数对象，并调用搜索服务。
2. **搜索服务**：调用外部搜索 API 获取初步搜索结果，并转换为内部统一的数据格式。
3. **AI 过滤服务**：利用大模型（如 Gemini）对搜索结果进行重排与过滤，挑选出最相关的网页。
4. **网页内容读取**：根据业务需求，通过 Playwright 或 Jina Reader API 进一步获取网页详细内容，提升大模型对内容理解的准确性。

该系统充分利用了大模型、AI 过滤和现代爬虫技术，为用户提供智能、高效的搜索结果处理与内容抓取解决方案。

---

通过本文档，你可以快速了解系统各模块的设计理念与实现细节，同时根据实际场景灵活选择合适的网页内容读取方案。
