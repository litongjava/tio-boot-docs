# 压力测试 - tio-boot

## 1. 测试概述

本文档主要通过 ApacheBench (ab) 和 wrk 两种工具，对基于 [tio-boot][^1] 搭建的本地服务器进行高并发访问压力测试，以评估其在大规模请求场景下的性能表现和稳定性。

---

## 2. 环境与测试代码

代码开源地址
https://github.com/litongjava/tio-boot-web-benchmarker

### 2.1 环境信息

- **服务器软件**：t-io
- **服务器主机名**：localhost
- **服务器端口**：80
- **操作系统**：Linux
- **JDK**：1.8
- **测试工具**：
  - ApacheBench (ab)
  - wrk

### 2.2 关键依赖

```xml
<dependencies>
  <dependency>
    <groupId>com.litongjava</groupId>
    <artifactId>tio-boot</artifactId>
    <version>${tio.boot.version}</version>
  </dependency>
  <dependency>
    <groupId>org.projectlombok</groupId>
    <artifactId>lombok</artifactId>
    <version>${lombok-version}</version>
    <scope>provided</scope>
  </dependency>
  <dependency>
    <groupId>com.alibaba.fastjson2</groupId>
    <artifactId>fastjson2</artifactId>
    <version>2.0.12</version>
  </dependency>
  <dependency>
    <groupId>com.litongjava</groupId>
    <artifactId>jfinal-aop</artifactId>
    <version>1.3.5/version>
  </dependency>
  <dependency>
    <groupId>com.litongjava</groupId>
    <artifactId>hotswap-classloader</artifactId>
    <version>${hotswap-classloader.version}</version>
  </dependency>
</dependencies>
```

### 2.3 启动类与测试接口

#### 启动类

```java
package com.litongjava.tio.web.hello;

import com.litongjava.annotation.AComponentScan;
import com.litongjava.tio.boot.TioApplication;

@AComponentScan
public class HelloApp {
  public static void main(String[] args) {
    long start = System.currentTimeMillis();
    TioApplication.run(HelloApp.class, args);
    long end = System.currentTimeMillis();
    System.out.println((end - start) + "ms");
  }
}
```

#### 配置类

```java
package com.litongjava.tio.web.hello.config;

import com.litongjava.annotation.AConfiguration;
import com.litongjava.annotation.Initialization;
import com.litongjava.tio.boot.server.TioBootServer;
import com.litongjava.tio.http.server.router.HttpRequestRouter;
import com.litongjava.tio.web.hello.handler.HelloHandler;

@AConfiguration
public class WebHelloConfig {

  @Initialization
  public void config() {

    TioBootServer server = TioBootServer.me();
    HttpRequestRouter requestRouter = server.getRequestRouter();

    HelloHandler helloHandler = new HelloHandler();
    requestRouter.add("/hello", helloHandler::hello);
  }
}

```

### handler

```java
package com.litongjava.tio.web.hello.handler;

import java.util.HashMap;
import java.util.Map;

import com.litongjava.model.body.RespBodyVo;
import com.litongjava.tio.boot.http.TioRequestContext;
import com.litongjava.tio.http.common.HttpRequest;
import com.litongjava.tio.http.common.HttpResponse;

public class HelloHandler {
  public HttpResponse hello(HttpRequest request) {
    // 如有需要，手动提取参数
    // 例如：String param = request.getParam("key");
    Map<String, String> data = new HashMap<>();
    RespBodyVo respVo = RespBodyVo.ok(data);
    return TioRequestContext.getResponse().setJson(respVo);
  }
}
```

## 3. 服务器配置

### 1. 系统资源检查

- **CPU 和内存**  
  确认服务器的 CPU 核心数和内存大小，确保有足够的资源应对高并发测试。
  ```bash
  lscpu
  free -m
  cat /proc/meminfo
  ```

#### lscpu

```
Architecture:             x86_64
  CPU op-mode(s):         32-bit, 64-bit
  Address sizes:          39 bits physical, 48 bits virtual
  Byte Order:             Little Endian
CPU(s):                   4
  On-line CPU(s) list:    0-3
Vendor ID:                GenuineIntel
  BIOS Vendor ID:         Intel(R) Corporation
  Model name:             Intel(R) Core(TM) i5-4210U CPU @ 1.70GHz
    BIOS Model name:      Intel(R) Core(TM) i5-4210U CPU @ 1.70GHz To Be Filled By O.E.M CPU @ 1.7GHz
    BIOS CPU family:      205
    CPU family:           6
    Model:                69
    Thread(s) per core:   2
    Core(s) per socket:   2
    Socket(s):            1
    Stepping:             1
    CPU(s) scaling MHz:   46%
    CPU max MHz:          2700.0000
    CPU min MHz:          800.0000
    BogoMIPS:             4789.06
    Flags:                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts
                          rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 movbe popcnt tsc_deadline_timer ae
                          s xsave avx f16c rdrand lahf_lm abm cpuid_fault epb invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms in
                          vpcid xsaveopt dtherm ida arat pln pts md_clear flush_l1d
Virtualization features:
  Virtualization:         VT-x
Caches (sum of all):
  L1d:                    64 KiB (2 instances)
  L1i:                    64 KiB (2 instances)
  L2:                     512 KiB (2 instances)
  L3:                     3 MiB (1 instance)
NUMA:
  NUMA node(s):           1
  NUMA node0 CPU(s):      0-3
Vulnerabilities:
  Gather data sampling:   Not affected
  Itlb multihit:          KVM: Mitigation: VMX disabled
  L1tf:                   Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable
  Mds:                    Mitigation; Clear CPU buffers; SMT vulnerable
  Meltdown:               Mitigation; PTI
  Mmio stale data:        Unknown: No mitigations
  Reg file data sampling: Not affected
  Retbleed:               Not affected
  Spec rstack overflow:   Not affected
  Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl
  Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization
  Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP conditional; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected
  Srbds:                  Mitigation; Microcode
  Tsx async abort:        Not affected
```

#### free -m

```
               total        used        free      shared  buff/cache   available
Mem:           15908         930       14715          16         547       14978
Swap:            975           0         975
```

#### cat /proc/meminfo

```
MemTotal:       16290700 kB
MemFree:        15070768 kB
MemAvailable:   15339752 kB
Buffers:           32960 kB
Cached:           481704 kB
SwapCached:            0 kB
Active:           159868 kB
Inactive:         837100 kB
Active(anon):       6580 kB
Inactive(anon):   492296 kB
Active(file):     153288 kB
Inactive(file):   344804 kB
Unevictable:         136 kB
Mlocked:               0 kB
SwapTotal:        999420 kB
SwapFree:         999420 kB
Zswap:                 0 kB
Zswapped:              0 kB
Dirty:                 0 kB
Writeback:             0 kB
AnonPages:        478452 kB
Mapped:           233044 kB
Shmem:             16572 kB
KReclaimable:      45744 kB
Slab:             104708 kB
SReclaimable:      45744 kB
SUnreclaim:        58964 kB
KernelStack:        3552 kB
PageTables:         4788 kB
SecPageTables:         0 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:     9144768 kB
Committed_AS:    5023368 kB
VmallocTotal:   34359738367 kB
VmallocUsed:       35612 kB
VmallocChunk:          0 kB
Percpu:             1904 kB
HardwareCorrupted:     0 kB
AnonHugePages:    233472 kB
ShmemHugePages:        0 kB
ShmemPmdMapped:        0 kB
FileHugePages:         0 kB
FilePmdMapped:         0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
Hugetlb:               0 kB
DirectMap4k:      180512 kB
DirectMap2M:     6025216 kB
DirectMap1G:    10485760 kB
```

### 2. 文件描述符和进程限制

- **文件描述符限制**  
  高并发连接时需要足够的文件描述符数量。查看当前限制：
  ```bash
  ulimit -n
  ```
  临时调整：
  ```bash
  ulimit -n 65535
  ```
  或修改 `/etc/security/limits.conf` 进行永久配置。

### 3. 网络参数调优

- **TCP 参数**  
  查看并根据需要调整 TCP 相关参数，比如最大连接数、TIME_WAIT 处理、socket 缓冲区大小等。  
  查看 TCP 参数：
  ```bash
  sysctl -a | grep net.ipv4.tcp
  ```
- **socket 队列**  
  检查并调整 `net.core.somaxconn` 参数来允许更多的挂起连接：
  ```bash
  sysctl net.core.somaxconn
  sudo sysctl -w net.core.somaxconn=4096
  ```
- **其他优化**  
  如 `net.ipv4.tcp_tw_reuse`、`net.ipv4.tcp_fin_timeout` 等参数，根据具体业务场景进行调优：
  ```bash
  sysctl -w net.ipv4.tcp_tw_reuse=2
  sysctl -w net.ipv4.tcp_fin_timeout=60
  ```

---

## 3. ApacheBench 压力测试

### 3.1 测试命令

```bash
ab -c1000 -n10000000 http://localhost/hello
```

- **并发级别（-c）**：1000
- **请求总数（-n）**：10,000,000

### 3.2 测试结果

```
This is ApacheBench, Version 2.3 <$Revision: 1913912 $>
Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
Licensed to The Apache Software Foundation, http://www.apache.org/

Benchmarking localhost (be patient)
Completed 1000000 requests
Completed 2000000 requests
Completed 3000000 requests
Completed 4000000 requests
Completed 5000000 requests
Completed 6000000 requests
Completed 7000000 requests
Completed 8000000 requests
Completed 9000000 requests
Completed 10000000 requests
Finished 10000000 requests


Server Software:        -io
Server Hostname:        localhost
Server Port:            80

Document Path:          /hello
Document Length:        54 bytes

Concurrency Level:      1000
Time taken for tests:   808.676 seconds
Complete requests:      10000000
Failed requests:        0
Total transferred:      1850000000 bytes
HTML transferred:       540000000 bytes
Requests per second:    12365.89 [#/sec] (mean)
Time per request:       80.868 [ms] (mean)
Time per request:       0.081 [ms] (mean, across all concurrent requests)
Transfer rate:          2234.07 [Kbytes/sec] received

Connection Times (ms)
              min  mean[+/-sd] median   max
Connect:        0   39   6.6     39     113
Processing:    13   42   7.3     41     306
Waiting:        7   30   7.3     29     278
Total:         48   81   6.1     79     309

Percentage of the requests served within a certain time (ms)
  50%     79
  66%     82
  75%     84
  80%     85
  90%     89
  95%     92
  98%     94
  99%     96
 100%    309 (longest request)
```

## Ab 报告分析

### 测试概览

- **并发水平 (Concurrency Level):** 1000  
  模拟了 1000 个并发连接，测试了服务在高并发场景下的响应能力。

- **总请求数 (Total requests):** 10,000,000  
  测试共发送了 1000 万次请求，数据量较大，有助于观察系统在长时间高负载下的表现。

- **测试耗时 (Time taken for tests):** 808.676 秒  
  完成所有请求共耗时约 808.7 秒，反映了整体处理时延。

- **失败请求 (Failed requests):** 0  
  无请求失败，显示系统在处理高并发请求时保持了较高的稳定性。

- **数据传输 (Total transferred / HTML transferred):**
  - 总传输数据量为 1,850,000,000 字节
  - HTML 传输数据量为 540,000,000 字节  
    这表明响应中包含的内容及 HTTP 头部的数据总量较大。

### 请求处理性能

- **每秒请求数 (Requests per second):** 12365.89 [#/sec]  
  表示服务每秒平均处理约 12366 个请求，体现了较高的吞吐量。

- **平均每个请求的处理时间 (Time per request):**
  - 单个请求平均耗时 80.868 毫秒
  - 而在所有并发请求中平均分摊时间仅为 0.081 毫秒  
    这两个指标从不同角度反映了请求的响应延迟情况。

### 数据传输效率

- **传输速率 (Transfer rate):** 2234.07 KB/秒  
  数据传输速率较高，说明在大流量数据传输时网络链路能够支持较大的带宽需求。

### 连接时间分析

各阶段连接时间的统计如下：

- **连接时间 (Connect):**

  - 最小：0 毫秒
  - 平均：39 毫秒（标准差 6.6 毫秒）
  - 中位数：39 毫秒
  - 最大：113 毫秒

- **处理时间 (Processing):**

  - 最小：13 毫秒
  - 平均：42 毫秒（标准差 7.3 毫秒）
  - 中位数：41 毫秒
  - 最大：306 毫秒

- **等待时间 (Waiting):**

  - 最小：7 毫秒
  - 平均：30 毫秒（标准差 7.3 毫秒）
  - 中位数：29 毫秒
  - 最大：278 毫秒

- **总时间 (Total):**
  - 最小：48 毫秒
  - 平均：81 毫秒（标准差 6.1 毫秒）
  - 中位数：79 毫秒
  - 最大：309 毫秒

这些数据表明，大部分请求的连接和处理时间都较为集中在较低的区间，但仍存在少数请求响应时间较长的情况（最大达到 309 毫秒）。

### 百分比分布

报告通过百分比分布进一步描述了响应时间的分布情况：

- **50%** 的请求响应时间不超过 79 毫秒
- **66%** 的请求响应时间不超过 82 毫秒
- **75%** 的请求响应时间不超过 84 毫秒
- **80%** 的请求响应时间不超过 85 毫秒
- **90%** 的请求响应时间不超过 89 毫秒
- **95%** 的请求响应时间不超过 92 毫秒
- **98%** 的请求响应时间不超过 94 毫秒
- **99%** 的请求响应时间不超过 96 毫秒
- **100%** 的请求响应时间不超过 309 毫秒

这些分布数据显示，绝大部分请求（超过 99%）在 96 毫秒内完成响应，仅极少部分请求可能会出现显著延迟。

## 4. wrk 压力测试

### 4.1 测试命令

```bash
wrk -t8 -c100 -d30s http://localhost/ok
```

- **线程数（-t）**：8
- **并发连接数（-c）**：100
- **压测时长（-d）**：30 秒

### 4.2 测试结果

```
Running 30s test @ http://localhost/ok
  8 threads and 100 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     5.52ms   12.28ms 422.98ms   92.53%
    Req/Sec     5.31k     0.97k   14.37k    77.97%
  1,268,823 requests in 30.10s, 211.76MB read
Requests/sec:  42154.00
Transfer/sec:      7.04MB
```

#### 4.2.1 关键指标

- **测试时长**：30 秒
- **总请求数**：1,268,823
- **平均请求速率**：42,154 [#/sec]
- **平均延迟**：5.52 ms
- **吞吐量（Transfer/sec）**：7.04 MB/s

#### 4.2.2 分析

1. **QPS 表现**：在 8 线程、100 并发连接下，QPS 超过 42k，展现了更高的吞吐能力。
2. **延迟表现**：平均延迟约 5.52 ms，说明对于当前并发压力，服务器仍能快速响应。
3. **可进一步提升的方向**：
   - **线程数与连接数调优**：结合实际硬件资源、网络环境，在 wrk 的参数上可进行不同组合测试。
   - **应用层面优化**：缓存、业务逻辑、序列化方式等都会影响处理效率。

---

## 5. 总结与建议

1. **高并发场景下的出色性能**

   - 从 `ab` 的 1000 并发、10,000,000 请求，到 `wrk` 的 8 线程、100 并发，都显示出较好的吞吐量和稳定性；多数请求在较短时间内完成。

---

> **备注**：以上测试数据仅代表在特定环境与参数配置下的结果，不同机器性能、网络环境以及操作系统配置都会对测试结果产生影响。建议在生产环境中进行更全面、持续的多维度压力测试，结合指标监控与调优，才能获得最优的性能表现。
