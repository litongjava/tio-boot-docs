# 增强检索（RAG）

增强检索（RAG）是一种通过结合大语言模型（LLM）和外部文档资料的检索方法，用以提升问题回答的准确性和相关性。它的核心在于将传统信息检索与大语言模型的生成能力相结合，为用户提供更为丰富的答案。RAG 的实现需要依赖向量数据库、文档切片、向量化等多种技术手段。以下是对增强检索、向量数据库及其实现方法的详细说明。

## 什么是增强检索（RAG）

增强检索（Retrieval-Augmented Generation，RAG）是一种结合了信息检索和生成技术的框架。其基本思想是先从大量文档库中检索出与用户查询相关的内容，再将检索到的信息作为上下文交给大语言模型，以生成最终的回答。相比于单纯依赖生成模型，RAG 能够通过检索外部资料，提高回答的精度和可信度。

### 工作原理

增强检索的工作原理如下：

1. **文档整理与格式化**：首先，收集并整理与目标主题相关的文档或知识，并对其进行结构化或格式化，以便更好地用于 AI 模型处理。
2. **文档切片**：将文档按段落或句子进行分割，使其能够进行更细粒度的处理，以提高后续检索的效率和准确性。
3. **向量化**：利用特定的嵌入模型（如 OpenAI 的嵌入模型），将分割后的文档段落转换成固定长度的向量，以便在向量数据库中存储和检索。
4. **用户查询向量化**：当用户提出查询时，系统将用户的问题进行向量化处理，生成与之对应的查询向量。
5. **向量预处理**：对查询向量进行标准化、去噪等预处理操作，以确保与文档向量的匹配效果最佳。
6. **向量数据库检索**：使用查询向量在向量数据库中进行相似性检索，找到与用户问题最相关的文档段落。
7. **构造提示模板**：将用户的问题和检索到的文档段落整合进提示模板中，构建合适的输入格式以发送给大语言模型。
8. **大语言模型处理**：将整合后的提示提交给大语言模型，如 ChatGPT，由模型根据提示生成最终的答案。
9. **答案生成**：大语言模型生成答案，并返回给用户。

### 运行流程

RAG 的运行流程可以简单地概括为以下几个步骤：

1. **接收请求**：首先，系统接收到用户的查询请求。
2. **信息检索（R)**：系统从一个预定义的大型文档库中检索出与用户查询最相关的文档片段。这一步通常使用向量检索算法来找到与查询最接近的文档。
3. **生成增强（A)**：将检索到的文档片段与原始查询问题一起输入到大语言模型中，并为生成过程提供额外的背景信息。
4. **输出生成（G)**：大语言模型基于文档和用户查询生成答案，最终将该答案返回给用户。

### 增强检索示例

以一个课程查询为例，说明 RAG 的实际操作流程：

1. 用户输入查询问题："Math 241 课程的期末考试时间是什么？"
2. 系统对用户查询进行重写，将其转换为："Math 241 期末考试时间"
3. 系统在向量数据库中检索与 "Math 241 期末考试时间" 相关的文档片段。
4. 将文档片段和用户查询组合在一起，输入到大语言模型中。
5. 大语言模型生成答案并返回，例如："Math 241 的期末考试时间是 2024 年 12 月 12 日上午 9 点。"

### 优势

增强检索的优势在于，它不仅能利用大语言模型生成答案，还能够从外部文档中补充事实信息。这使得它在处理需要精确事实或数据的任务时，表现优异。它适用于需要丰富上下文或精确信息的场景，如法律、医学、技术文档查询等。

## 向量数据库（Chroma）

向量数据库是增强检索中不可或缺的一部分，它用于存储和管理文档的嵌入向量。通过向量检索算法，可以快速、高效地找到与用户查询相关的文档片段。

Chroma 是一种流行的开源向量数据库，支持持久化存储和高效检索。它可以与多种嵌入模型结合使用，用于处理文本、图像等不同类型的数据。

### 安装 Chroma

以下是如何在不同操作系统上安装和运行 Chroma 的说明。

#### Windows

1. 安装 Python，并配置好环境变量。安装方法请参考[官方文档](https://docs.python.org/zh-cn/3/using/windows.html#the-full-installer)。
2. 验证安装是否成功：
   ```bash
   python --version
   ```
3. 安装 Chroma，参考[官方指南](https://docs.trychroma.com/getting-started)。
4. 验证 Chroma 是否成功运行：
   ```bash
   chroma run
   ```

#### Mac

1. 使用 Homebrew 安装 Python：
   ```bash
   brew install python
   ```
2. 验证 Python 是否正确安装：
   ```bash
   python --version
   ```
3. 安装并运行 Chroma，参考 [官方指南](https://docs.trychroma.com/getting-started)：
   ```bash
   chroma run
   ```

#### Linux

与 Mac 类似，可以通过包管理器安装 Python，然后安装 Chroma。

#### Docker 部署

可以使用 Docker 快速部署 Chroma：

```bash
mkdir /opt/docker/chromadb
cd /opt/docker/chromadb
docker run -d --restart=always --name chromadb -v $(pwd)/chroma:/chroma/chroma -p 8000:8000 -e IS_PERSISTENT=TRUE -e ANONYMIZED_TELEMETRY=TRUE chromadb/chroma:latest
```

启动成功后，可以访问 `http://localhost:8000` 来查看 Chroma 的运行状态。

### 使用 Chroma

启动 Chroma 后，可以通过向量化文档段落，将其存储到 Chroma 的向量数据库中。然后，可以使用向量化的用户查询在 Chroma 数据库中检索与之相似的文档片段。

## LangChain4j

LangChain4j 是 LangChain 的 Java 版本。LangChain 是一个面向大语言模型开发的框架，旨在简化与 LLM 的集成，并通过链式执行的方式将不同模块（如信息检索、推理生成）串联起来。

LangChain4j 提供了文档加载、切片、嵌入、存储及检索的全流程支持，简化了构建增强检索系统的流程。

### 添加依赖

要在项目中使用 LangChain4j，首先需要在 Maven 中添加相应的依赖：

```xml
<properties>
  <langchain4j.version>0.31.0</langchain4j.version>
</properties>

<dependency>
  <groupId>dev.langchain4j</groupId>
  <artifactId>langchain4j</artifactId>
  <version>${langchain4j.version}</version>
</dependency>
<dependency>
  <groupId>dev.langchain4j</groupId>
  <artifactId>langchain4j-open-ai</artifactId>
   <version>${langchain4j.version}</version>
</dependency>
<dependency>
  <groupId>dev.langchain4j</groupId>
  <artifactId>langchain4j-chroma</artifactId>
   <version>${langchain4j.version}</version>
</dependency>
<dependency>
  <groupId>com.litongjava</groupId>
  <artifactId>java-openai</artifactId>
  <version>1.0.3</version>
</dependency>
```

### 文档切分

由于大语言模型的 Token 输入限制，为了确保模型能够有效处理文档中的知识，需要将较大的文档进行合理的切分，并将切分后的内容存储在向量库中。在与 LLM 交互时，先通过向量检索获取与用户问题相关的文本段，再将其作为上下文输入给模型进行推理和回答。

为了更好地进行检索，文档通常会被拆分成更小的段落或句子。LangChain4j 提供了多种切分策略，开发者可以根据实际需求选择合适的方案。

以下是一个使用 LangChain4j 进行文档切片的示例代码：

```java
import dev.langchain4j.data.document.Document;
import dev.langchain4j.data.document.DocumentSplitter;
import dev.langchain4j.data.document.loader.FileSystemDocumentLoader;
import dev.langchain4j.data.document.splitter.DocumentSplitters;
import dev.langchain4j.model.openai.OpenAiTokenizer;

import java.io.InputStream;
import java.net.URL;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.List;

public class LangChain4JLoadDocumentSplitTest {

    public static void main(String[] args) throws Exception {
        // 下载文件
        String fileUrl = "https://raw.githubusercontent.com/litongjava/tio-boot-docs/main/docs/zh/01_tio-boot%20%E7%AE%80%E4%BB%8B/02.md";
        Path tempFilePath = Paths.get("downloaded.md");

        try (InputStream in = new URL(fileUrl).openStream()) {
            Files.copy(in, tempFilePath);
        }

        // 加载文档
        Document document = FileSystemDocumentLoader.loadDocument(tempFilePath);

        // 切分文档
        DocumentSplitter splitter = DocumentSplitters.recursive(150, 10, new OpenAiTokenizer());
        List<TextSegment> segments = splitter.split(document);

        // 输出切分后的文档段落数量
        System.out.println("切分后的段落数量: " + segments.size());

        // 删除临时文件
        Files.delete(tempFilePath);
    }
}
```

#### 文档拆分的原理

文档的每个段落或句子在进行分割时，通常需要考虑文本的大小、重叠度以及文本中可能存在的语义中断问题。特别是在与大语言模型进行交互时，文本的长度限制是一个重要因素。常见的 LLM 模型如 GPT-3.5、GPT-4，以及其他模型对输入 Token 的数量有明确的上限。为了避免输入过长导致的处理延迟或报错问题，文档通常需要进行切分并通过向量库进行管理和检索。

#### LangChain4j 提供的文档拆分方案

在增强检索系统中，LangChain4j 提供了多种文档拆分策略，开发者可以根据文档内容选择适合的拆分方案：

1. **基于字符的拆分（DocumentByCharacterSplitter）**：逐个字符进行分割，包括空白字符。
2. **基于行的拆分（DocumentByLineSplitter）**：按换行符（`\n`）进行分割，适用于结构化文本，如代码或诗歌。
3. **基于段落的拆分（DocumentByParagraphSplitter）**：根据连续的两个换行符（`\n\n`）分割段落，适用于较为连续的长文档。
4. **基于正则表达式的拆分（DocumentByRegexSplitter）**：根据正则表达式进行自定义分割，可灵活处理特殊文本格式。
5. **基于句子的拆分（DocumentBySentenceSplitter）**：基于句子结构进行分割，通常使用自然语言处理工具（如 Apache OpenNLP），适用于英文文本。
6. **基于字的拆分（DocumentByWordSplitter）**：将文本按照空白字符进行分割，主要适用于单词分割。

#### 文档拆分的实现流程

文档拆分的流程可以通过递归方式进行处理，最终输出的是一个分段后的文本列表。以下是一个文档拆分的典型流程：

1. 首先使用段落拆分器将文档划分为多个段落。
2. 每个段落再进一步按照句子、行或字符等方式进行细化处理。
3. 在保证文本段 Token 数量不超过设定值的前提下，逐步生成分段文本。
4. 对于相邻的段落，通过设置重叠 Token 数来减少文本语义的中断，确保模型在处理这些分段时能够连续理解上下文。

```java
DocumentSplitter splitter = DocumentSplitters.recursive(150, 10, new OpenAiTokenizer());
List<TextSegment> segments = splitter.split(document);
```

上述代码通过 `DocumentSplitters.recursive()` 方法实现递归式文档拆分，接受三个参数：

- **分段大小**：一个文本段最大包含的 Token 数量。
- **重叠度**：相邻段落之间的重叠 Token 数量。
- **分词器**：用于计算文本的 Token 数量，如 `OpenAiTokenizer`。

递归的拆分方式确保在满足 Token 数量限制的前提下，保持文本语义的完整性和连续性。

**图解流程**：

1. 首先通过段落切分，将长文档切分为多个较大的段落部分（如：A、B、C 段）。
2. 然后对每个段落再进行更精细的拆分（如句子、行等）。
3. 在各个段落间使用 Token 重叠技术（如：A1-A2、B1-B2）保持语义连贯。

这一流程确保了文档内容被完整传递到大语言模型中，同时避免了单次输入超过 Token 限制的风险。
[01](images/01.png)

##### Token 的概念

Token 是文本中最小的单位，经过分词器（Tokenizer）的处理后，文本被切分为词或子词。在中文中，一个字符通常对应 1-2 个 Token，而英文文本则可能由多个字符组成一个 Token。以 "我喜欢吃苹果" 为例，可以拆分为 "我/喜欢/吃/苹果" 这种形式，得到 4 个 Token，也可以进一步细分为 "我/喜/欢/吃/苹果"，得到 5 个 Token。

LLM 在进行推理时，输入的 Token 数量直接影响到其响应能力。虽然理论上某些模型可以支持输入数百万个字符，但实际应用中通常建议控制在数千到几万个 Token 以内。下表列举了常见大语言模型的 Token 上限：

| 模型名称                   | Token 输入上限 |
| -------------------------- | -------------- |
| GPT-3 (davinci)            | 4096 tokens    |
| GPT-3.5 (text-davinci-003) | 4096 tokens    |
| GPT-4 (8k context)         | 8192 tokens    |
| GPT-4 (32k context)        | 32768 tokens   |
| LLaMA (7B/13B/30B/65B)     | 2048 tokens    |
| 讯飞星火（SparkDesk）      | 8192 tokens    |
| 文心一言（Ernie 3.0）      | 4096 tokens    |
| 智源悟道（WuDao 2.0）      | 2048 tokens    |
| 阿里巴巴 M6                | 2048 tokens    |
| 华为盘古（Pangu-Alpha）    | 2048 tokens    |
| 京东言犀大模型（ChatJd）   | 2048 tokens    |

因此，当我们与大语言模型交互时，文本的 Token 总数不得超出模型的限制，否则可能导致模型无法处理或性能大幅下降。

#### OpenAiTokenizer

文档拆分时不会调用 OpenAI 的接口。`OpenAiTokenizer` 只是一个本地的分词器，它模拟了 OpenAI 生成 Token 的方式，用于在本地将文档拆分成 Token 片段。因此，在文档拆分过程中，不会涉及任何外部 API 调用或网络请求。

具体来说，`OpenAiTokenizer` 的作用是根据 OpenAI 使用的分词算法（如 BPE，Byte Pair Encoding）对文本进行本地的 Token 化，从而确保切分的文档片段与 OpenAI 的 Token 限制保持一致。在你进行文档拆分时，所有的处理都会在本地完成，而不是通过调用 OpenAI 的接口。

因此，你可以放心地进行文档切分，而不必担心在这个过程中会调用 OpenAI 的外部服务。

#### Token 长度的重要性

根据不同模型的 Token 上限，我们需要严格控制单次输入的文本 Token 数量。例如 GPT-4 的 32k Token 上限，意味着如果输入的文本长度超过了该限制，模型可能会忽略部分内容或返回错误。因此，在实际操作中，字符串长度建议不超过 64k 字符，以确保系统性能。

### 向量化

为了在向量数据库中存储文档，我们需要将文档段落向量化。以下是使用 LangChain4j 将文本段落嵌入为向量的示例：
由于需要将已拆分的知识片段文本存储向量库以便后续可以进行检索，而向量库存储的数据是向量不是文本，因此需要将文本进行向量化，即将一个字符串转换为一个 N 维数组，这个过程在自然语言处理（NLP）领域称为文本嵌入（Words Embedding）。
不同的 LLM 对于文本嵌入的实现是不同的，ChatGPT 的实现是基于 transformer 架构的，相关实现存储在服务端，每次嵌入都需要访问 OpenAI 的 HTTP 接口。

```java
import dev.langchain4j.data.embedding.Embedding;
import dev.langchain4j.model.openai.OpenAiEmbeddingModel;
import dev.langchain4j.model.openai.OpenAiEmbeddingModelName;

public class LangChain4JEmbeddingTest {

    public static void main(String[] args) {
        // 配置 OpenAI API 密钥
        String openApiKey = "YOUR_OPENAI_API_KEY";

        // 创建嵌入模型
        OpenAiEmbeddingModel embeddingModel = new OpenAiEmbeddingModel.OpenAiEmbeddingModelBuilder()
                .apiKey(openApiKey)
                .modelName(OpenAiEmbeddingModelName.TEXT_EMBEDDING_3_LARGE)
                .build();

        // 需要嵌入的文本
        String text = "两只眼睛";

        // 生成嵌入
        Embedding embedding = embeddingModel.embed(text).content();

        // 输出嵌入结果
        System.out.println("文本的嵌入结果: " + embedding.vectorAsList());
        System.out.println("向量维度: " + embedding.dimension());
    }
}
```

```
2024-09-07 13:56:44.686 [main] INFO  c.l.t.LangChain4JEmbeddingTest.test:22 - 当前的模型是: text-embedding-3-large
2024-09-07 13:56:45.958 [main] INFO  c.l.t.LangChain4JEmbeddingTest.test:25 - 文本:两只眼睛的嵌入结果是:
[0.010431603, -0.010523479, -0.0020089315, -0.024707913, 0.002698011, 0.015972508, 0.01518095, 0.043705303, -0.0061911135, -0.018644016, -6.731776E-4, ....]
```

## 增强检索（RAG）示例

向量数据库，也称为向量存储或向量搜索引擎，是专门设计用于存储和管理向量（固定长度的数字列表）及相关数据的数据库。这些向量通常表示数据点在高维空间中的数学特征，每个维度对应数据的某个特征。向量数据库的主要目的是通过近似最近邻（Approximate Nearest Neighbors，ANN）算法实现高效的相似性搜索。

在构建一个基于增强检索（RAG）的系统时，首先需要启动 ChromaDB，连接 LangChain4j 的 SDK 创建一个数据存储容器（集合，类似于 MySQL 的表）。一旦嵌入过程完成，便可以将向量（Embedding）和文本段落（TextSegment）存储在向量库中。

### 向量化与检索

为了在向量数据库中进行查询，输入的查询文本也需要进行向量化处理。接下来，系统通过 ANN 算法检索与该查询向量最相似的文档片段，并根据匹配的文档生成最终的回答。

查询过程通常涉及以下四个输入参数：

1. **查询嵌入向量（queryEmbedding）**：表示查询文本的向量化结果。
2. **最大检索数量（maxResults）**：指定检索时返回的最近邻文档数量。
3. **最小分值（minScore）**：用于过滤低于指定相似度的结果。
4. **元数据过滤器**：根据文档元数据进行额外的过滤。

为了有效地将检索结果传递给大语言模型，通常需要定义一个提示模板，告诉 LLM 基于检索到的文档信息生成回答。以下是一个示例提示模板：

```text
基于如下信息进行回答:\n{{context}}\n提问:\n{{question}}
```

### 示例代码

下面是一个完整的代码示例，展示如何使用 LangChain4j、Chroma 向量库以及 OpenAI API 进行增强检索，并生成答案。

```java
package com.litongjava.test;

import java.io.IOException;
import java.io.InputStream;
import java.math.BigInteger;
import java.net.MalformedURLException;
import java.net.URL;
import java.nio.charset.StandardCharsets;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.security.MessageDigest;
import java.security.NoSuchAlgorithmException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.junit.Test;

import com.litongjava.openai.chat.ChatResponseVo;
import com.litongjava.openai.client.OpenAiClient;
import com.litongjava.tio.utils.environment.EnvUtils;
import com.litongjava.tio.utils.json.JsonUtils;

import dev.langchain4j.data.document.Document;
import dev.langchain4j.data.document.DocumentSplitter;
import dev.langchain4j.data.document.loader.FileSystemDocumentLoader;
import dev.langchain4j.data.document.splitter.DocumentSplitters;
import dev.langchain4j.data.embedding.Embedding;
import dev.langchain4j.data.message.UserMessage;
import dev.langchain4j.data.segment.TextSegment;
import dev.langchain4j.model.input.Prompt;
import dev.langchain4j.model.input.PromptTemplate;
import dev.langchain4j.model.openai.OpenAiTokenizer;
import dev.langchain4j.store.embedding.EmbeddingMatch;
import dev.langchain4j.store.embedding.EmbeddingSearchRequest;
import dev.langchain4j.store.embedding.EmbeddingSearchRequest.EmbeddingSearchRequestBuilder;
import dev.langchain4j.store.embedding.EmbeddingSearchResult;
import dev.langchain4j.store.embedding.EmbeddingStore;
import dev.langchain4j.store.embedding.chroma.ChromaEmbeddingStore;
import dev.langchain4j.store.embedding.filter.Filter;
import dev.langchain4j.store.embedding.filter.comparison.IsEqualTo;

public class Langchain4jChromDbTest {

  @Test
  public void test() throws NoSuchAlgorithmException {
    // 加载环境变量
    EnvUtils.load();

    // 下载待处理的文件
    String fileUrl = "https://raw.githubusercontent.com/litongjava/tio-boot-docs/main/docs/zh/01_tio-boot%20%E7%AE%80%E4%BB%8B/02.md";
    Path tempFilePath = Paths.get("downloaded.md");

    // 下载并保存文件
    try {
      Files.deleteIfExists(tempFilePath); // 若文件已存在，先删除
    } catch (IOException e) {
      e.printStackTrace();
    }
    try (InputStream in = new URL(fileUrl).openStream()) {
      Files.copy(in, tempFilePath);
    } catch (MalformedURLException e) {
      e.printStackTrace();
    } catch (IOException e) {
      e.printStackTrace();
    }

    // 加载文档
    Document document = FileSystemDocumentLoader.loadDocument(tempFilePath);

    // 使用 OpenAI Tokenizer 切分文档
    DocumentSplitter splitter = DocumentSplitters.recursive(150, 10, new OpenAiTokenizer());
    List<TextSegment> segments = splitter.split(document);

    System.out.println("拆分后的片段数量:" + segments.size());

    // 设置 Chroma 向量数据库 URL 和集合名
    String CHROMA_URL = EnvUtils.get("CHROMA_URL");
    String CHROMA_DB_DEFAULT_COLLECTION_NAME = "tio-boot";

    // 创建向量存储器并将向量与文本段存储
    EmbeddingStore<TextSegment> embeddingStore = ChromaEmbeddingStore.builder().baseUrl(CHROMA_URL).collectionName(CHROMA_DB_DEFAULT_COLLECTION_NAME).build();

    // MD5 哈希生成器
    MessageDigest md = MessageDigest.getInstance("MD5");
    EmbeddingSearchRequestBuilder builder = EmbeddingSearchRequest.builder();
    for (TextSegment segment : segments) {
      // 为文本段生成 MD5 哈希
      String segmentText = segment.text();
      String segmentMd5 = new BigInteger(1, md.digest(segmentText.getBytes(StandardCharsets.UTF_8))).toString(16);

      // 在 Chroma 中通过 MD5 进行搜索，而不是生成新的嵌入
      Map<String, String> metadataFilter = new HashMap<>();
      metadataFilter.put("md5", segmentMd5);
      // 创建一个 Filter 来搜索 MD5 匹配的条目
      Filter md5Filter = new IsEqualTo("md5", segmentMd5);

      float[] dummyVector = new float[3072]; // 创建一个 3072 维的空向量，默认值为 0.0f
      Embedding dummyEmbedding = new Embedding(dummyVector); // 用这个向量创建一个虚拟的 Embedding

      EmbeddingSearchRequest searchRequest = builder.filter(md5Filter) // 通过元数据 (MD5) 进行搜索
          .queryEmbedding(dummyEmbedding) // 传入虚拟 Embedding
          .maxResults(1) // 搜索最多返回1个结果
          .build();
      EmbeddingSearchResult<TextSegment> searchResult = embeddingStore.search(searchRequest);

      // 如果存在相同的 MD5，跳过添加
      boolean isDuplicate = !searchResult.matches().isEmpty();
      if (isDuplicate) {
        System.out.println("跳过重复片段: " + segmentMd5);
        continue;
      }

      // 将文本段嵌入并存储，同时存储 MD5 作为元数据
      float[] vector = OpenAiClient.embeddingArrayByLargeModel(segmentText);
      Embedding embedding = new Embedding(vector); // 使用 MD5 作为元数据
      segment.metadata().put("md5", segmentMd5);
      embeddingStore.add(embedding, segment);
    }

    // 用户查询问题
    String question = "如何启动tio-boot";
    // 使用 API 生成用户查询的嵌入向量
    Embedding queryEmbedding = new Embedding(OpenAiClient.embeddingArrayByLargeModel(question));

    // 在 Chroma 中检索最相关的文档段
    EmbeddingSearchRequest embeddingSearchRequest = builder.queryEmbedding(queryEmbedding).maxResults(1).build();
    EmbeddingSearchResult<TextSegment> embeddedEmbeddingSearchResult = embeddingStore.search(embeddingSearchRequest);

    // 获取最相关的文档段
    List<EmbeddingMatch<TextSegment>> embeddingMatches = embeddedEmbeddingSearchResult.matches();
    List<String> context = new ArrayList<>();
    for (EmbeddingMatch<TextSegment> match : embeddingMatches) {
      TextSegment textSegment = match.embedded();
      context.add(textSegment.text());
    }

    // 构造 Prompt 模板
    PromptTemplate promptTemplate = PromptTemplate.from("基于如下信息进行回答:\n{{context}}\n提问:\n{{question}}");
    Map<String, Object> variables = new HashMap<>();
    variables.put("context", JsonUtils.toJson(context));
    variables.put("question", question);
    Prompt prompt = promptTemplate.apply(variables);

    // 构建用户消息并向 OpenAI 提交请求
    UserMessage userMessage = prompt.toUserMessage();
    String singleText = userMessage.singleText();
    // 输出生成的回答
    ChatResponseVo chatResponse = OpenAiClient.chatCompletionsWithRole("user", singleText);
    // 输出信息
    System.out.println(singleText);
    System.out.println(JsonUtils.toJson(chatResponse));
  }
}

```

### 示例讲解

1. **文档加载和切分**：程序首先下载目标文档，并使用 LangChain4j 的 `DocumentSplitter` 进行切分。切分后的每个文档段落将用于后续向量化处理。
2. **嵌入和存储**：切分后的文档段落通过 OpenAI 的嵌入模型生成向量，并存储到 Chroma 向量库中，供后续检索使用。
3. **用户查询处理**：用户提出问题后，系统将查询问题向量化，并在 Chroma 向量库中检索最相关的文档段。
4. **生成答案**：根据检索到的文档段落，构建提示模板并提交给大语言模型，生成最终答案并返回给用户。

input

````
基于如下信息进行回答:
["```json\n{ \"data\": {}, \"code\": 1, \"msg\": null, \"ok\": true }\n```\n\n### 使用 spring-boot 插件启动\n\n```shell\nmvn spring-boot:run -Pproduction\n```\n\n访问测试 http://localhost/,显示 index\n\n### 打包\n\n```shell\nmvn clean package -DskipTests -Pproduction\n```\n\n### 启动\n\n```shell\njava -jar target\\**.jar\n```"]
提问:
如何启动tio-boot
````

output

````json
{
  "object": "chat.completion",
  "id": "chatcmpl-A54QPwujlLRgB2EZeQxO8rHoWebUh",
  "model": "gpt-4o-mini-2024-07-18",
  "choices": [
    {
      "message": {
        "content": "根据您提供的信息，启动 `tio-boot` 的步骤如下：\n\n1. **使用 Spring Boot 插件启动**：\n   在项目根目录下，运行以下命令：\n   ```shell\n   mvn spring-boot:run -Pproduction\n   ```\n\n2. **访问测试**：\n   启动后，可以访问 [http://localhost/](http://localhost/) 来查看是否显示 `index` 页面。\n\n3. **打包**：\n   如果您想打包项目，可以使用以下命令：\n   ```shell\n   mvn clean package -DskipTests -Pproduction\n   ```\n\n4. **启动已打包的应用**：\n   打包完成后，运行以下命令来启动应用：\n   ```shell\n   java -jar target\\**.jar\n   ```\n   记得将 `**.jar` 替换为实际生成的 JAR 文件名。\n\n根据这些步骤，您可以顺利启动 `tio-boot` 应用。",
        "role": "assistant",
        "tool_calls": null
      },
      "index": 0,
      "delta": null,
      "logprobs": null,
      "finish_reason": "stop"
    }
  ],
  "created": "1725772013",
  "usage": {
    "prompt_tokens": 135,
    "total_tokens": 347,
    "completion_tokens": 212
  },
  "system_fingerprint": "fp_483d39d857"
}
````

## 总结

增强检索（RAG）是一种高效的信息查询方法，结合了信息检索和大语言模型的推理能力。通过文档切分、嵌入生成和向量检索，RAG 可以从海量文档中提取最相关的信息，并根据上下文生成高质量的回答。LangChain4j 提供了完整的工具链，帮助开发者简化该过程，并应用于各种实际场景，如文档问答、智能搜索等。
